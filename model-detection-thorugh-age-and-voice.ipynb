{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2024-10-24T09:49:35.238571Z","iopub.status.busy":"2024-10-24T09:49:35.238188Z","iopub.status.idle":"2024-10-24T09:49:51.069035Z","shell.execute_reply":"2024-10-24T09:49:51.067824Z","shell.execute_reply.started":"2024-10-24T09:49:35.238528Z"},"trusted":true},"outputs":[],"source":["# importing important libraries for need in work for bulding CNN model\n","import os\n","import numpy as np\n","import pandas as pd\n","import cv2\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n","from sklearn.model_selection import train_test_split\n","from tensorflow.keras.utils import to_categorical"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-10-24T09:56:42.861111Z","iopub.status.busy":"2024-10-24T09:56:42.860645Z","iopub.status.idle":"2024-10-24T09:56:44.024752Z","shell.execute_reply":"2024-10-24T09:56:44.023465Z","shell.execute_reply.started":"2024-10-24T09:56:42.861071Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Contents of UTKFace:\n","['26_0_2_20170104023102422.jpg.chip.jpg', '22_1_1_20170112233644761.jpg.chip.jpg', '21_1_3_20170105003215901.jpg.chip.jpg', '28_0_0_20170117180555824.jpg.chip.jpg', '17_1_4_20170103222931966.jpg.chip.jpg', '44_0_3_20170119201022260.jpg.chip.jpg', '35_0_2_20170116182734834.jpg.chip.jpg', '76_0_0_20170104213515132.jpg.chip.jpg', '36_1_0_20170116165722892.jpg.chip.jpg', '34_0_3_20170119200815948.jpg.chip.jpg']\n","Contents of utkface_aligned_cropped:\n","['UTKFace', 'crop_part1']\n","Contents of crop_part1:\n","['26_0_2_20170104023102422.jpg.chip.jpg', '21_1_3_20170105003215901.jpg.chip.jpg', '17_1_4_20170103222931966.jpg.chip.jpg', '76_0_0_20170104213515132.jpg.chip.jpg', '18_1_0_20170104022856102.jpg.chip.jpg', '12_1_0_20170109203700059.jpg.chip.jpg', '4_0_0_20170110212940437.jpg.chip.jpg', '67_0_0_20170111193821219.jpg.chip.jpg', '26_1_2_20170104021513981.jpg.chip.jpg', '90_0_2_20170111210740854.jpg.chip.jpg']\n"]}],"source":["# Check contents of subdirectories and which directory have images\n","subdirs = [\"UTKFace\", \"utkface_aligned_cropped\", \"crop_part1\"]\n","\n","for subdir in subdirs:\n","    full_path = os.path.join(data_dir, subdir)\n","    print(f\"Contents of {subdir}:\")\n","    print(os.listdir(full_path)[:10])  # Print first 10 files in each subdirectory\n","# we can see utk face and crop part 1 contain images\n","# so lets proceed with utk face"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-10-24T10:02:47.204044Z","iopub.status.busy":"2024-10-24T10:02:47.202878Z","iopub.status.idle":"2024-10-24T10:02:47.209137Z","shell.execute_reply":"2024-10-24T10:02:47.207763Z","shell.execute_reply.started":"2024-10-24T10:02:47.203989Z"},"trusted":true},"outputs":[],"source":["# Set the directory to the 'UTKFace' subfolder\n","data_dir = \"/kaggle/input/utkface-new/UTKFace\""]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2024-10-24T10:11:59.117007Z","iopub.status.busy":"2024-10-24T10:11:59.116613Z","iopub.status.idle":"2024-10-24T10:13:38.148639Z","shell.execute_reply":"2024-10-24T10:13:38.147324Z","shell.execute_reply.started":"2024-10-24T10:11:59.116967Z"},"trusted":true},"outputs":[],"source":["images = []\n","ages = []\n","\n","for img_name in os.listdir(data_dir):\n","    if img_name.endswith(\".jpg\") or img_name.endswith(\".png\"):  # Include other extensions if necessary\n","        try:\n","            # Extract age from the filename (assuming age is the first part of the filename)\n","            age = int(img_name.split('_')[0])\n","            img_path = os.path.join(data_dir, img_name)\n","            \n","            # Read and process the image\n","            img = cv2.imread(img_path)\n","            if img is not None:\n","                img = cv2.resize(img, (64, 64))  # Resize to 64x64\n","                images.append(img)\n","                ages.append(age)\n","            else:\n","                print(f\"Could not read image: {img_name}\")\n","        except Exception as e:\n","            print(f\"Error processing file {img_name}: {e}\")\n"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2024-10-24T10:16:08.276041Z","iopub.status.busy":"2024-10-24T10:16:08.275526Z","iopub.status.idle":"2024-10-24T10:16:08.282212Z","shell.execute_reply":"2024-10-24T10:16:08.281031Z","shell.execute_reply.started":"2024-10-24T10:16:08.275994Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Number of images loaded: 23708\n","Number of ages loaded: 23708\n"]}],"source":["# Check the number of images and ages loaded\n","print(f\"Number of images loaded: {len(images)}\")\n","print(f\"Number of ages loaded: {len(ages)}\")\n"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2024-10-24T10:17:37.838634Z","iopub.status.busy":"2024-10-24T10:17:37.838183Z","iopub.status.idle":"2024-10-24T10:17:37.998614Z","shell.execute_reply":"2024-10-24T10:17:37.997455Z","shell.execute_reply.started":"2024-10-24T10:17:37.838590Z"},"trusted":true},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","\n","# Convert the lists to numpy arrays for training\n","import numpy as np\n","X = np.array(images)\n","y = np.array(ages)"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2024-10-24T10:18:01.014460Z","iopub.status.busy":"2024-10-24T10:18:01.013988Z","iopub.status.idle":"2024-10-24T10:18:01.152458Z","shell.execute_reply":"2024-10-24T10:18:01.151342Z","shell.execute_reply.started":"2024-10-24T10:18:01.014414Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Training set size: (18966, 64, 64, 3), Test set size: (4742, 64, 64, 3)\n"]}],"source":["# Split the data\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","print(f\"Training set size: {X_train.shape}, Test set size: {X_test.shape}\")"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2024-10-24T10:21:26.020095Z","iopub.status.busy":"2024-10-24T10:21:26.019631Z","iopub.status.idle":"2024-10-24T10:21:26.145241Z","shell.execute_reply":"2024-10-24T10:21:26.144062Z","shell.execute_reply.started":"2024-10-24T10:21:26.020052Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n","  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"]}],"source":["# Sample CNN Model for Age Detection\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n","\n","model = Sequential([\n","    Conv2D(32, (3, 3), activation='relu', input_shape=(64, 64, 3)),\n","    MaxPooling2D(2, 2),\n","    Conv2D(64, (3, 3), activation='relu'),\n","    MaxPooling2D(2, 2),\n","    Flatten(),\n","    Dense(128, activation='relu'),\n","    Dense(1, activation='linear')  # Output for age prediction\n","])"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2024-10-24T10:21:48.956958Z","iopub.status.busy":"2024-10-24T10:21:48.956509Z","iopub.status.idle":"2024-10-24T10:32:45.985233Z","shell.execute_reply":"2024-10-24T10:32:45.983790Z","shell.execute_reply.started":"2024-10-24T10:21:48.956913Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/10\n","\u001b[1m593/593\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 90ms/step - loss: 6324.8027 - mae: 23.6897 - val_loss: 205.6154 - val_mae: 10.2181\n","Epoch 2/10\n","\u001b[1m593/593\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 90ms/step - loss: 142.2805 - mae: 8.8914 - val_loss: 139.3751 - val_mae: 8.5795\n","Epoch 3/10\n","\u001b[1m593/593\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 89ms/step - loss: 116.4540 - mae: 8.0255 - val_loss: 142.4325 - val_mae: 8.4744\n","Epoch 4/10\n","\u001b[1m593/593\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 91ms/step - loss: 105.1056 - mae: 7.7186 - val_loss: 125.2507 - val_mae: 8.0681\n","Epoch 5/10\n","\u001b[1m593/593\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 91ms/step - loss: 93.4321 - mae: 7.2438 - val_loss: 126.0436 - val_mae: 8.1421\n","Epoch 6/10\n","\u001b[1m593/593\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 91ms/step - loss: 88.3062 - mae: 7.0095 - val_loss: 126.4752 - val_mae: 8.0707\n","Epoch 7/10\n","\u001b[1m593/593\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 92ms/step - loss: 80.0012 - mae: 6.7157 - val_loss: 128.3187 - val_mae: 8.1395\n","Epoch 8/10\n","\u001b[1m593/593\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 91ms/step - loss: 76.0674 - mae: 6.5501 - val_loss: 120.1900 - val_mae: 7.9050\n","Epoch 9/10\n","\u001b[1m593/593\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 92ms/step - loss: 68.9313 - mae: 6.1903 - val_loss: 123.7479 - val_mae: 7.9355\n","Epoch 10/10\n","\u001b[1m593/593\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m87s\u001b[0m 101ms/step - loss: 66.6675 - mae: 6.1364 - val_loss: 131.7611 - val_mae: 8.0950\n"]},{"data":{"text/plain":["<keras.src.callbacks.history.History at 0x7a68bc9d3250>"]},"execution_count":15,"metadata":{},"output_type":"execute_result"}],"source":["model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mae'])\n","# training  the model with train dataset\n","model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10)\n","# evaluating model performance using (mae- mean absoulute error)"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2024-10-24T10:33:30.300352Z","iopub.status.busy":"2024-10-24T10:33:30.299795Z","iopub.status.idle":"2024-10-24T10:33:30.395095Z","shell.execute_reply":"2024-10-24T10:33:30.393711Z","shell.execute_reply.started":"2024-10-24T10:33:30.300298Z"},"trusted":true},"outputs":[],"source":["#now we are going to Save this model with h.5 extension\n","model.save(\"age_detection_cnn.h5\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# now we are going to do task2\n","# the task was age detection through voice\n","# force the voice detection want to go with librosa"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"datasetId":44109,"sourceId":78156,"sourceType":"datasetVersion"}],"dockerImageVersionId":30786,"isGpuEnabled":false,"isInternetEnabled":false,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"}},"nbformat":4,"nbformat_minor":4}
